//! Post-codegen checks that can highlight potential issues with the generated bytecode.

use std::{collections::HashMap, fmt::Display, ops::Range};

use acvm::{
    AcirField,
    acir::brillig::{
        BitSize, BlackBoxOp, HeapArray, HeapVector, IntegerBitSize, MemoryAddress, Opcode,
        ValueOrArray,
    },
};

use crate::{
    brillig::{
        brillig_gen::brillig_fn::FunctionContext,
        brillig_ir::{
            BrilligContext, ReservedRegisters,
            artifact::{BrilligArtifact, LabelType, OpcodeLocation},
            registers::{RegisterAllocator, Stack},
        },
    },
    ssa::{
        ir::{basic_block::BasicBlockId, function::Function},
        opt::Loops,
    },
};

pub(crate) enum OpcodeAdvisory {
    /// A memory address is being written by the opcode that no other following opcode reads.
    NeverRead { addr: MemoryAddress },
    /// A memory address is being written by the opcode that will get overwritten before being read.
    OverwrittenBeforeRead { addr: MemoryAddress, write_at: OpcodeLocation, read_at: OpcodeLocation },
}

pub(crate) type OpcodeAdvisories = HashMap<OpcodeLocation, Vec<OpcodeAdvisory>>;

/// Go through opcodes and collect advisories, indicating opcodes which could potentially be removed
pub(crate) fn opcode_advisories<F: AcirField>(
    function: &Function,
    function_context: &FunctionContext,
    brillig_context: &BrilligContext<F, Stack>,
) -> OpcodeAdvisories {
    // Find where each block start and ends in the bytecode.
    let block_opcode_ranges = block_opcode_ranges(brillig_context);

    // Accumulate stack addresses read. These are all in the same function context,
    // so they should fit in one stack frame, which makes relative addresses comparable.
    let registers = brillig_context.registers();
    let addr_range = Range { start: registers.start(), end: registers.end() };

    // First collect all reads per block.
    let reads_in_blocks: HashMap<BasicBlockId, im::HashSet<MemoryAddress>> = function_context
        .reverse_post_order()
        .map(|block_id| {
            let mut read_collector = ReadCollector::new(&addr_range);
            let opcode_range = block_opcode_ranges[&block_id].clone();
            for loc in opcode_range {
                let opcode = &brillig_context.artifact().byte_code[loc];
                read_collector.visit_opcode(opcode, loc);
            }
            (block_id, read_collector.into_reads())
        })
        .collect();

    // Find each loop in the function. If a block is part of a loop and writes to an address,
    // we need to consider all blocks that we can get to from the header as a potential descendant.
    let blocks_in_loops: HashMap<BasicBlockId, _> = Loops::find_all(function)
        .yet_to_unroll
        .into_iter()
        .map(|loop_| (loop_.header, loop_.blocks))
        .collect();

    // Collect all the reads that happen in the descendants of blocks.
    let mut reads_in_descendants: HashMap<BasicBlockId, im::HashSet<MemoryAddress>> =
        HashMap::default();

    // Going in Post Order, so successors will have been processed already,
    // except for back-edges: the header of the loop they return to comes later.
    for block_id in function_context.post_order() {
        // The reads are a union of addresses read:
        // * directly by the successors
        // * indirectly by the descendants of the successors
        let mut acc = im::HashSet::new();
        for successor_id in function.dfg[block_id].successors() {
            acc = acc.union(reads_in_blocks[&successor_id].clone());
            acc = acc.union(reads_in_descendants[&successor_id].clone());
        }
        // If the block is a header of a loop, then anything we can reach from the header,
        // we can reach from any block in the loop too.
        // For example say we have a CFG like this, b1 is the header of a loop [b1, b2, b3]
        //      +-------+
        //      v       |
        // b0--b1--b2--b3
        //      +----------b4
        // An address can written in b3 and read in b1 (e.g. the loop variable),
        // or written in b2 and read in b4 (e.g. a reference allocated in b0).
        if let Some(blocks_in_loop) = blocks_in_loops.get(&block_id) {
            for member_id in blocks_in_loop {
                if *member_id == block_id {
                    continue;
                }
                // Blocks in the loop are successors, they should be done already.
                let member_reads = reads_in_descendants[member_id].clone();
                reads_in_descendants.insert(*member_id, member_reads.union(acc.clone()));
            }
        }
        // We are done with this node, can move on to its predecessors.
        reads_in_descendants.insert(block_id, acc);
    }

    // Now that we have the reads in the blocks and their descendants, look at the writes.
    let mut advisories = HashMap::new();
    for block_id in function_context.reverse_post_order() {
        let mut advisory_collector =
            AdvisoryCollector::new(&addr_range, &reads_in_descendants[&block_id]);
        let opcode_range = block_opcode_ranges[&block_id].clone();

        // Going backwards, so reads at the end are recorded before earlier writes.
        for loc in opcode_range.rev() {
            let opcode = &brillig_context.artifact().byte_code[loc];
            if advisory_collector.should_visit_opcode(opcode) {
                advisory_collector.visit_opcode(opcode, loc);
            }
        }

        advisories.extend(advisory_collector.into_advisories());
    }
    advisories
}

/// Display the opcodes and their corresponding advisories.
pub(crate) fn show_opcode_advisories<F: Display>(
    advisories: &OpcodeAdvisories,
    artifact: &BrilligArtifact<F>,
) {
    println!(
        "\n// There are {} Brillig opcode advisories for function '{}'",
        advisories.len(),
        artifact.name
    );

    if advisories.is_empty() {
        return;
    }

    let advisory = |opcode: &Opcode<F>, msg: &str| {
        let mut a = format!("{opcode}");
        if msg.is_empty() {
            return a;
        }
        for _ in a.len()..40 {
            a.push(' ');
        }
        a.push_str(" // ");
        a.push_str(msg);
        a
    };

    println!("fn {}", artifact.name);
    let index_width = artifact.byte_code.len().to_string().len();
    for (index, opcode) in artifact.byte_code.iter().enumerate() {
        let msg = if let Some(ads) = advisories.get(&index) {
            &ads.iter()
                .map(|a| match a {
                    OpcodeAdvisory::NeverRead { addr } => format!("{addr} is never read"),
                    OpcodeAdvisory::OverwrittenBeforeRead { addr, write_at, read_at } => {
                        format!("{addr} is overwritten at {write_at} before read at {read_at}")
                    }
                })
                .collect::<Vec<_>>()
                .join("; ")
        } else {
            ""
        };
        println!("{index:>index_width$}: {}", advisory(opcode, msg));
    }
}

/// Go through the labels in the [BrilligContext] and collect the opcode locations
/// where each block starts and ends.
fn block_opcode_ranges<F, R: RegisterAllocator>(
    brillig_context: &BrilligContext<F, R>,
) -> HashMap<BasicBlockId, Range<OpcodeLocation>> {
    let mut ranges = HashMap::new();

    let mut labels = brillig_context.artifact().labels.iter().collect::<Vec<_>>();
    labels.sort_by_key(|(_, location)| **location);

    let mut last_start = None;
    for (label, location) in labels {
        match (&label.label_type, last_start) {
            (LabelType::Function(_, Some(block_id)), None) => {
                last_start = Some((*block_id, *location));
            }
            (LabelType::Function(_, Some(new_block_id)), Some((block_id, start))) => {
                if *new_block_id != block_id {
                    ranges.insert(block_id, Range { start, end: *location });
                    last_start = Some((*new_block_id, *location));
                }
            }
            (_, Some((block_id, start))) => {
                ranges.insert(block_id, Range { start, end: *location });
                last_start = None;
            }
            (_, None) => {}
        }
    }
    if let Some((block_id, start)) = last_start {
        ranges.insert(block_id, Range { start, end: brillig_context.artifact().byte_code.len() });
    }

    ranges
}

/// Collect all the relative addresses read in a range.
struct ReadCollector<'a> {
    addr_range: &'a Range<OpcodeLocation>,
    reads: im::HashSet<MemoryAddress>,
}

impl<'a> ReadCollector<'a> {
    fn new(addr_range: &'a Range<usize>) -> Self {
        Self { addr_range, reads: im::HashSet::new() }
    }

    fn into_reads(self) -> im::HashSet<MemoryAddress> {
        self.reads
    }
}

impl OpcodeAddressVisitor for ReadCollector<'_> {
    fn read(&mut self, addr: &MemoryAddress, _location: OpcodeLocation) {
        if addr.is_relative() && self.addr_range.contains(&addr.to_usize()) {
            self.reads.insert(*addr);
        }
    }

    fn write(&mut self, _addr: &MemoryAddress, _location: OpcodeLocation) {
        // Ignore writes.
    }

    fn should_visit_opcode<F>(&mut self, _opcode: &Opcode<F>) -> bool {
        true
    }
}

/// Collect advisories about the opcodes in a single block.
///
/// Assumes that we are visiting the opcodes going backwards.
struct AdvisoryCollector<'a> {
    addr_range: &'a Range<OpcodeLocation>,
    /// Addresses read in any of the descendants of the block.
    reads_in_descendants: &'a im::HashSet<MemoryAddress>,
    /// Last location the address was read from in this block.
    reads: HashMap<MemoryAddress, OpcodeLocation>,
    /// Last location the address was written to in this block.
    writes: HashMap<MemoryAddress, OpcodeLocation>,
    /// Advisories collected in this block.
    advisories: OpcodeAdvisories,
    /// Indicate that we are in a region where call parameters are being passed.
    /// * 2 means we are in a call region
    /// * 1 means we are 1 step before the start of the call region
    /// * 0 means we are not in a call region
    in_call_region: u8,
    /// Indicate that we are in the region where we are passing out return parameters.
    in_return_region: bool,
}

impl<'a> AdvisoryCollector<'a> {
    fn new(
        addr_range: &'a Range<usize>,
        reads_in_descendants: &'a im::HashSet<MemoryAddress>,
    ) -> Self {
        Self {
            addr_range,
            reads_in_descendants,
            reads: HashMap::new(),
            writes: HashMap::new(),
            advisories: HashMap::new(),
            in_call_region: 0,
            in_return_region: false,
        }
    }

    fn into_advisories(self) -> OpcodeAdvisories {
        self.advisories
    }

    fn addr_in_range(&self, addr: &MemoryAddress) -> bool {
        addr.is_relative() && self.addr_range.contains(&addr.to_usize())
    }

    fn add_advisory(&mut self, location: OpcodeLocation, advisory: OpcodeAdvisory) {
        self.advisories.entry(location).or_default().push(advisory);
    }
}

impl OpcodeAddressVisitor for AdvisoryCollector<'_> {
    /// Maintain some contextual information about regions.
    fn should_visit_opcode<F: AcirField>(&mut self, opcode: &Opcode<F>) -> bool {
        // The return instruction can be preceded by a number of moves from various parts of the stack
        // to the beginning, depending on the number of return values. They destinations can be shuffled.
        if matches!(opcode, Opcode::Return) {
            self.in_return_region = true;
        } else if !matches!(opcode, Opcode::Mov { .. }) {
            self.in_return_region = false;
        }

        match opcode {
            // Handle the conventions of `codegen_call`: we are passing call arguments by coping them
            // to new stack registers, then we set the stack pointer @0, which changes the meaning of
            // relative addresses. The parameters are going to be read by another function, not visible
            // in the scope of a single function we are analyzing here.
            Opcode::Mov { destination, source } => {
                if *destination == ReservedRegisters::stack_pointer()
                    && *source == MemoryAddress::relative(0)
                {
                    // This is the restore of the stack pointer, the end of a call.
                    self.in_call_region = 2;
                    false
                } else if self.in_call_region == 2 && *source == ReservedRegisters::stack_pointer()
                {
                    // This is where we store the stack pointer in the first address of the next stack frame.
                    // The next opcode is a const that contains the current stack size.
                    self.in_call_region = 1;
                    false
                } else {
                    // We can visit the Mov instructions that copy parameters and return values;
                    // we may need to ignore the destination, but we can still register the reads.
                    true
                }
            }
            Opcode::Const { .. } if self.in_call_region == 1 => {
                // This is the instruction where we set the stack size at the beginning of the call.
                self.in_call_region = 0;
                true
            }
            // A `constrain` of some arbitrary expression is usually broken up into an instruction with
            // a boolean result, and then a constrain on the result being equal to 1. Constant allocation
            // creates a constant for that 1, but the codegen later might not use it, but rather check
            // the variable directly, since it's already a bool value.
            Opcode::Const {
                destination: _,
                bit_size: BitSize::Integer(IntegerBitSize::U1),
                value,
            } => !value.is_one(),
            _ => true,
        }
    }

    /// Remember the opcode location where a memory address was last read.
    fn read(&mut self, addr: &MemoryAddress, location: OpcodeLocation) {
        if !self.addr_in_range(addr) {
            return;
        }
        self.reads.insert(*addr, location);
    }

    /// Remember the opcode location where a memory address was last read.
    ///
    /// Insert an advisory if:
    /// * the address is not read after this opcode
    /// * the address is read after this, but before that there is another write
    fn write(&mut self, addr: &MemoryAddress, location: OpcodeLocation) {
        if !self.addr_in_range(addr) {
            return;
        }
        if self.in_call_region != 0 || self.in_return_region {
            // Ignore the write, it's a parameter meant for another function.
            // The reads can be inspected, as they should consume data we have prepared.
            return;
        }
        if let Some(read_at) = self.reads.get(addr) {
            if let Some(write_at) = self.writes.get(addr) {
                if write_at < read_at {
                    self.add_advisory(
                        location,
                        OpcodeAdvisory::OverwrittenBeforeRead {
                            addr: *addr,
                            write_at: *write_at,
                            read_at: *read_at,
                        },
                    );
                }
            }
        } else if !self.reads_in_descendants.contains(addr) {
            self.add_advisory(location, OpcodeAdvisory::NeverRead { addr: *addr });
        }
        self.writes.insert(*addr, location);
    }
}

trait OpcodeAddressVisitor {
    /// Decide if an opcode should be visited.
    ///
    /// Can mutate self to set flags that allows it to ignore upcoming opcodes as well.
    fn should_visit_opcode<F: AcirField>(&mut self, opcode: &Opcode<F>) -> bool;

    /// Called with all the addresses read by the opcode.
    fn read(&mut self, addr: &MemoryAddress, location: OpcodeLocation);

    /// Called with all addresses written by the opcode.
    fn write(&mut self, addr: &MemoryAddress, location: OpcodeLocation);

    /// Expected to be called with each opcode, traversing blocks in Post Order,
    /// feeding the opcodes back to front.
    fn visit_opcode<F: AcirField>(&mut self, opcode: &Opcode<F>, location: OpcodeLocation) {
        if !self.should_visit_opcode(opcode) {
            return;
        }
        match opcode {
            Opcode::BinaryFieldOp { destination, lhs, rhs, .. }
            | Opcode::BinaryIntOp { destination, lhs, rhs, .. } => {
                self.read(lhs, location);
                self.read(rhs, location);
                self.write(destination, location);
            }
            Opcode::Cast { destination, source, .. }
            | Opcode::Mov { destination, source }
            | Opcode::Not { destination, source, .. } => {
                self.read(source, location);
                self.write(destination, location);
            }
            Opcode::Load { destination, source_pointer } => {
                self.read(source_pointer, location);
                self.write(destination, location);
            }
            Opcode::Store { destination_pointer, source, .. } => {
                self.read(destination_pointer, location); // the write goes where the pointer points
                self.read(source, location);
            }
            Opcode::JumpIf { condition, .. } => {
                self.read(condition, location);
            }
            Opcode::CalldataCopy { destination_address, size_address, offset_address } => {
                self.read(size_address, location);
                self.read(offset_address, location);
                self.write(destination_address, location);
            }
            Opcode::Const { destination, .. } => {
                self.write(destination, location);
            }
            Opcode::IndirectConst { destination_pointer, .. } => {
                self.read(destination_pointer, location);
            }
            Opcode::ForeignCall { destinations, inputs, .. } => {
                for input in inputs {
                    self.read_value_or_array(input, location);
                }
                for destination in destinations {
                    // Consider the destinations as something the VM will read;
                    // we prepared these values for it, even if we don't use them later.
                    self.read_value_or_array(destination, location);
                    // We can also consider them being written to.
                    self.write_value_or_array(destination, location);
                }
            }
            Opcode::ConditionalMov { source_a, source_b, condition, .. } => {
                self.read(condition, location);
                self.read(source_a, location);
                self.read(source_b, location);
            }
            Opcode::BlackBox(black_box_op) => self.visit_black_box_op(black_box_op, location),
            Opcode::Trap { revert_data } => self.read_heap_vector(revert_data, location),
            Opcode::Stop { return_data } => self.read_heap_vector(return_data, location),
            Opcode::Jump { .. } | Opcode::Call { .. } | Opcode::Return { .. } => {}
        }
    }

    fn read_heap_array(&mut self, array: &HeapArray, location: OpcodeLocation) {
        self.read(&array.pointer, location);
    }

    fn read_heap_vector(&mut self, vector: &HeapVector, location: OpcodeLocation) {
        self.read(&vector.pointer, location);
        self.read(&vector.size, location);
    }

    fn read_value_or_array(&mut self, value: &ValueOrArray, location: OpcodeLocation) {
        match value {
            ValueOrArray::MemoryAddress(memory_address) => self.read(memory_address, location),
            ValueOrArray::HeapArray(heap_array) => self.read_heap_array(heap_array, location),
            ValueOrArray::HeapVector(heap_vector) => self.read_heap_vector(heap_vector, location),
        }
    }

    fn write_heap_array(&mut self, array: &HeapArray, location: OpcodeLocation) {
        self.write(&array.pointer, location);
    }

    fn write_heap_vector(&mut self, vector: &HeapVector, location: OpcodeLocation) {
        self.write(&vector.pointer, location);
        self.write(&vector.size, location);
    }

    fn write_value_or_array(&mut self, value: &ValueOrArray, location: OpcodeLocation) {
        match value {
            ValueOrArray::MemoryAddress(memory_address) => self.write(memory_address, location),
            ValueOrArray::HeapArray(heap_array) => self.write_heap_array(heap_array, location),
            ValueOrArray::HeapVector(heap_vector) => self.write_heap_vector(heap_vector, location),
        }
    }

    fn visit_black_box_op(&mut self, op: &BlackBoxOp, location: OpcodeLocation) {
        match op {
            BlackBoxOp::AES128Encrypt { inputs, iv, key, outputs } => {
                self.read_heap_vector(inputs, location);
                self.read_heap_array(iv, location);
                self.read_heap_array(key, location);
                self.write_heap_vector(outputs, location);
            }
            BlackBoxOp::Blake2s { message, output } | BlackBoxOp::Blake3 { message, output } => {
                self.read_heap_vector(message, location);
                self.write_heap_array(output, location);
            }
            BlackBoxOp::Keccakf1600 { input, output } => {
                self.read_heap_array(input, location);
                self.write_heap_array(output, location);
            }
            BlackBoxOp::EcdsaSecp256k1 {
                hashed_msg,
                public_key_x,
                public_key_y,
                signature,
                result,
            }
            | BlackBoxOp::EcdsaSecp256r1 {
                hashed_msg,
                public_key_x,
                public_key_y,
                signature,
                result,
            } => {
                self.read_heap_vector(hashed_msg, location);
                self.read_heap_array(public_key_x, location);
                self.read_heap_array(public_key_y, location);
                self.read_heap_array(signature, location);
                self.write(result, location);
            }
            BlackBoxOp::MultiScalarMul { points, scalars, outputs } => {
                self.read_heap_vector(points, location);
                self.read_heap_vector(scalars, location);
                self.write_heap_array(outputs, location);
            }
            BlackBoxOp::EmbeddedCurveAdd {
                input1_x,
                input1_y,
                input1_infinite,
                input2_x,
                input2_y,
                input2_infinite,
                result,
            } => {
                self.read(input1_x, location);
                self.read(input1_y, location);
                self.read(input1_infinite, location);
                self.read(input2_x, location);
                self.read(input2_y, location);
                self.read(input2_infinite, location);
                self.write_heap_array(result, location);
            }
            BlackBoxOp::Poseidon2Permutation { message, output } => {
                self.read_heap_vector(message, location);
                self.write_heap_array(output, location);
            }
            BlackBoxOp::Sha256Compression { input, hash_values, output } => {
                self.read_heap_array(input, location);
                self.read_heap_array(hash_values, location);
                self.write_heap_array(output, location);
            }
            BlackBoxOp::ToRadix { input, radix, output_pointer, num_limbs, output_bits } => {
                self.read(input, location);
                self.read(radix, location);
                self.read(num_limbs, location);
                self.read(output_bits, location);
                self.read(output_pointer, location); // indirect
            }
        }
    }
}
