use std::collections::BTreeMap;
use std::sync::{Arc, Mutex, RwLock};

use acvm::{FieldElement, acir::AcirField};
use iter_extended::vecmap;
use noirc_errors::Location;
use noirc_frontend::ast::BinaryOpKind;
use noirc_frontend::monomorphization::ast::{
    self, FuncId, GlobalId, InlineType, LocalId, Parameters, Program,
};
use noirc_frontend::shared::Signedness;
use noirc_frontend::signed_field::SignedField;

use crate::errors::RuntimeError;
use crate::ssa::function_builder::FunctionBuilder;
use crate::ssa::ir::basic_block::BasicBlockId;
use crate::ssa::ir::function::FunctionId as IrFunctionId;
use crate::ssa::ir::function::{Function, RuntimeType};
use crate::ssa::ir::instruction::BinaryOp;
use crate::ssa::ir::map::AtomicCounter;
use crate::ssa::ir::types::{NumericType, Type};
use crate::ssa::ir::value::ValueId;

use super::GlobalsGraph;
use super::value::{Tree, Value, Values};
use rustc_hash::FxHashMap as HashMap;

/// The FunctionContext is the main context object for translating a
/// function into SSA form during the SSA-gen pass.
///
/// This context can be used to build any amount of functions,
/// so long as it is cleared out in between each function via
/// calling self.new_function().
///
/// If compiling many functions across multiple threads, there should
/// be a separate FunctionContext for each thread. Each FunctionContext
/// can communicate via the SharedContext field which as its name suggests
/// is the only part of the context that needs to be shared between threads.
pub(super) struct FunctionContext<'a> {
    definitions: HashMap<LocalId, Values>,

    pub(super) builder: FunctionBuilder,
    shared_context: &'a SharedContext,

    /// Contains any loops we're currently in the middle of translating.
    /// These are ordered such that an inner loop is at the end of the vector and
    /// outer loops are at the beginning. When a loop is finished, it is popped.
    loops: Vec<Loop>,
}

/// Shared context for all functions during ssa codegen. This is the only
/// object that is shared across all threads when generating ssa in multiple threads.
///
/// The main job of the SharedContext is to remember which functions are already
/// compiled, what their IDs are, and keep a queue of which functions still need to
/// be compiled.
///
/// SSA can be generated by continuously popping from this function_queue and using
/// FunctionContext to generate from the popped function id. Once the queue is empty,
/// no other functions are reachable and the SSA generation is finished.
pub(super) struct SharedContext {
    /// All currently known functions which have already been assigned function ids.
    /// These functions are all either currently having their SSA generated or are
    /// already finished.
    functions: RwLock<HashMap<FuncId, IrFunctionId>>,

    /// Queue of which functions still need to be compiled.
    ///
    /// The queue is currently Last-in First-out (LIFO) but this is an
    /// implementation detail that can be trivially changed and should
    /// not impact the resulting SSA besides changing which IDs are assigned
    /// to which functions.
    function_queue: Mutex<FunctionQueue>,

    /// Shared counter used to assign the ID of the next function
    function_counter: AtomicCounter<Function>,

    /// A pseudo function that represents global values.
    /// Globals are only concerned with the values and instructions (due to Instruction::MakeArray)
    /// in a function's DataFlowGraph. However, in order to re-use various codegen methods
    /// we need to use the same `Function` type.
    pub(super) globals_context: Function,

    pub(super) globals: BTreeMap<GlobalId, Values>,

    /// The entire monomorphized source program
    pub(super) program: Program,
}

#[derive(Copy, Clone)]
pub(super) struct Loop {
    pub(super) loop_entry: BasicBlockId,
    /// The loop index will be `Some` for a `for` and `None` for a `loop`
    pub(super) loop_index: Option<ValueId>,
    pub(super) loop_end: BasicBlockId,
}

/// The queue of functions remaining to compile
type FunctionQueue = Vec<(FuncId, IrFunctionId)>;

impl<'a> FunctionContext<'a> {
    /// Create a new FunctionContext to compile the first function in the shared_context's
    /// function queue.
    ///
    /// This will pop from the function queue, so it is expected the shared_context's function
    /// queue is non-empty at the time of calling this function. This can be ensured by calling
    /// `shared_context.get_or_queue_function(function_to_queue)` before calling this constructor.
    ///
    /// `function_name` and `parameters` are expected to be the name and parameters of the function
    /// this constructor will pop from the function queue.
    pub(super) fn new(
        function_name: String,
        parameters: &Parameters,
        runtime: RuntimeType,
        shared_context: &'a SharedContext,
        globals: GlobalsGraph,
    ) -> Self {
        let function_id = shared_context
            .pop_next_function_in_queue()
            .expect("No function in queue for the FunctionContext to compile")
            .1;

        let mut builder = FunctionBuilder::new(function_name, function_id);
        builder.set_globals(Arc::new(globals));
        builder.set_runtime(runtime);

        let definitions = HashMap::default();
        let mut this = Self { definitions, builder, shared_context, loops: Vec::new() };
        this.add_parameters_to_scope(parameters);
        this
    }

    /// Finish building the current function and switch to building a new function with the
    /// given name, id, and parameters.
    ///
    /// Note that the previous function cannot be resumed after calling this. Developers should
    /// avoid calling new_function until the previous function is completely finished with ssa-gen.
    pub(super) fn new_function(&mut self, id: IrFunctionId, func: &ast::Function) {
        self.definitions.clear();

        if func.unconstrained {
            self.builder.new_brillig_function(func.name.clone(), id, func.inline_type);
        } else {
            self.builder.new_function(func.name.clone(), id, func.inline_type);
        }

        self.add_parameters_to_scope(&func.parameters);
    }

    /// Add each parameter to the current scope, and return the vector of parameter types.
    ///
    /// The returned parameter type vector will be flattened, so any struct parameters will
    /// be returned as one entry for each field (recursively).
    fn add_parameters_to_scope(&mut self, parameters: &Parameters) {
        for (id, mutable, _, typ, _visibility) in parameters {
            self.add_parameter_to_scope(*id, typ, *mutable);
        }
    }

    /// Adds a "single" parameter to scope.
    ///
    /// Single is in quotes here because in the case of tuple parameters, the tuple is flattened
    /// into a new parameter for each field recursively.
    fn add_parameter_to_scope(
        &mut self,
        parameter_id: LocalId,
        parameter_type: &ast::Type,
        mutable: bool,
    ) {
        // Add a separate parameter for each field type in 'parameter_type'
        let parameter_value = Self::map_type(parameter_type, |typ| {
            let value = self.builder.add_parameter(typ);
            if mutable {
                // This will wrap any `mut var: T` in a reference
                self.new_mutable_variable(value)
            } else {
                value.into()
            }
        });

        self.definitions.insert(parameter_id, parameter_value);
    }

    /// Allocate a single slot of memory and store into it the given initial value of the variable.
    /// Always returns a Value::Mutable wrapping the allocate instruction.
    pub(super) fn new_mutable_variable(&mut self, value_to_store: ValueId) -> Value {
        let element_type = self.builder.current_function.dfg.type_of_value(value_to_store);

        let alloc = self.builder.insert_allocate(element_type);
        self.builder.insert_store(alloc, value_to_store);
        let typ = self.builder.type_of_value(value_to_store);
        Value::Mutable(alloc, typ)
    }

    /// Maps the given type to a Tree of the result type.
    ///
    /// This can be used to (for example) flatten a tuple type, creating
    /// and returning a new parameter for each field type.
    pub(super) fn map_type<T>(typ: &ast::Type, mut f: impl FnMut(Type) -> T) -> Tree<T> {
        Self::map_type_helper(typ, &mut f)
    }

    // This helper is needed because we need to take f by mutable reference,
    // otherwise we cannot move it multiple times each loop of vecmap.
    fn map_type_helper<T>(typ: &ast::Type, f: &mut dyn FnMut(Type) -> T) -> Tree<T> {
        match typ {
            ast::Type::Tuple(fields) => {
                Tree::Branch(vecmap(fields, |field| Self::map_type_helper(field, f)))
            }
            ast::Type::Unit => Tree::empty(),
            // A mutable reference wraps each element into a reference.
            // This can be multiple values if the element type is a tuple.
            ast::Type::Reference(element, _) => {
                Self::map_type_helper(element, &mut |typ| f(Type::Reference(Arc::new(typ))))
            }
            ast::Type::FmtString(len, fields) => {
                // A format string is represented by multiple values
                // The message string, the number of fields to be formatted, and
                // then the encapsulated fields themselves
                let final_fmt_str_fields =
                    vec![ast::Type::String(*len), ast::Type::Field, *fields.clone()];
                let fmt_str_tuple = ast::Type::Tuple(final_fmt_str_fields);
                Self::map_type_helper(&fmt_str_tuple, f)
            }
            ast::Type::Vector(elements) => {
                let element_types = Self::convert_type(elements).flatten();
                Tree::Branch(vec![
                    Tree::Leaf(f(Type::length_type())),
                    Tree::Leaf(f(Type::Vector(Arc::new(element_types)))),
                ])
            }
            other => Tree::Leaf(f(Self::convert_non_tuple_type(other))),
        }
    }

    /// Convert a monomorphized type to an SSA type, preserving the structure
    /// of any tuples within.
    pub(super) fn convert_type(typ: &ast::Type) -> Tree<Type> {
        // Do nothing in the closure here - map_type_helper already calls
        // convert_non_tuple_type internally.
        Self::map_type_helper(typ, &mut |x| x)
    }

    /// Converts a non-tuple type into an SSA type. Panics if a tuple type is passed.
    ///
    /// This function is needed since this SSA IR has no concept of tuples and thus no type for
    /// them. Use `convert_type` if tuple types need to be handled correctly.
    pub(super) fn convert_non_tuple_type(typ: &ast::Type) -> Type {
        match typ {
            ast::Type::Field => Type::field(),
            ast::Type::Array(len, element) => {
                let element_types = Self::convert_type(element).flatten();
                Type::Array(Arc::new(element_types), *len)
            }
            ast::Type::Integer(Signedness::Signed, bits) => Type::signed((*bits).into()),
            ast::Type::Integer(Signedness::Unsigned, bits) => Type::unsigned((*bits).into()),
            ast::Type::Bool => Type::unsigned(1),
            ast::Type::String(len) => Type::str(*len),
            ast::Type::FmtString(_, _) => {
                panic!("convert_non_tuple_type called on a fmt string: {typ}")
            }
            ast::Type::Unit => panic!("convert_non_tuple_type called on a unit type"),
            ast::Type::Tuple(_) => panic!("convert_non_tuple_type called on a tuple: {typ}"),
            ast::Type::Function(_, _, _, _) => Type::Function,
            ast::Type::Vector(_) => panic!("convert_non_tuple_type called on a vector: {typ}"),
            ast::Type::Reference(element, _) => {
                // Recursive call to panic if element is a tuple
                let element = Self::convert_non_tuple_type(element);
                Type::Reference(Arc::new(element))
            }
        }
    }

    /// Returns the unit value, represented as an empty tree of values
    pub(super) fn unit_value() -> Values {
        Values::empty()
    }

    /// Insert a numeric constant into the current function
    ///
    /// Unlike FunctionBuilder::numeric_constant, this version checks the given constant
    /// is within the range of the given type. This is needed for user provided values where
    /// otherwise values like 2^128 can be assigned to a u8 without error or wrapping.
    pub(super) fn checked_numeric_constant(
        &mut self,
        value: SignedField,
        numeric_type: NumericType,
    ) -> Result<ValueId, RuntimeError> {
        if let Some(range) = numeric_type.value_is_outside_limits(value) {
            let call_stack = self.builder.get_call_stack();
            return Err(RuntimeError::IntegerOutOfBounds {
                value,
                typ: numeric_type,
                range,
                call_stack,
            });
        }

        let value = if value.is_negative() {
            match numeric_type {
                NumericType::NativeField => -value.absolute_value(),
                NumericType::Signed { bit_size } | NumericType::Unsigned { bit_size } => {
                    assert!(bit_size < 128);
                    let base = 1_u128 << bit_size;
                    FieldElement::from(base) - value.absolute_value()
                }
            }
        } else {
            value.absolute_value()
        };

        Ok(self.builder.numeric_constant(value, numeric_type))
    }

    /// Insert a binary instruction at the end of the current block.
    /// Converts the form of the binary instruction as necessary
    /// (e.g. swapping arguments, inserting a not) to represent it in the IR.
    /// For example, (a <= b) is represented as !(b < a)
    pub(super) fn insert_binary(
        &mut self,
        mut lhs: ValueId,
        operator: BinaryOpKind,
        mut rhs: ValueId,
        location: Location,
    ) -> Values {
        let op = convert_operator(operator);
        if operator_requires_swapped_operands(operator) {
            std::mem::swap(&mut lhs, &mut rhs);
        }

        self.builder.set_location(location);

        let mut result = self.builder.insert_binary(lhs, op, rhs);

        if operator_requires_not(operator) {
            result = self.builder.insert_not(result);
        }

        result.into()
    }

    /// Inserts a call instruction at the end of the current block and returns the results
    /// of the call.
    ///
    /// Compared to self.builder.insert_call, this version will reshape the returned `Vec<ValueId>`
    /// back into a Values tree of the proper shape.
    pub(super) fn insert_call(
        &mut self,
        function: ValueId,
        arguments: Vec<ValueId>,
        result_type: &ast::Type,
        location: Location,
    ) -> Values {
        let result_types = Self::convert_type(result_type).flatten();
        let results =
            self.builder.set_location(location).insert_call(function, arguments, result_types);

        let mut i = 0;
        let reshaped_return_values = Self::map_type(result_type, |_| {
            let result = results[i].into();
            i += 1;
            result
        });
        assert_eq!(i, results.len());
        reshaped_return_values
    }

    /// Inserts a cast instruction at the end of the current block and returns the results
    /// of the cast.
    ///
    /// Compared to `self.builder.insert_cast`, this version will automatically truncate `value` to be a valid `typ`.
    pub(super) fn insert_safe_cast(
        &mut self,
        value: ValueId,
        typ: NumericType,
        location: Location,
    ) -> ValueId {
        self.builder.set_location(location);
        let incoming_type = self.builder.type_of_value(value);

        let result = match (&incoming_type, typ) {
            // Casting to field is safe
            (_, NumericType::NativeField) => value,
            (
                Type::Numeric(NumericType::Signed { bit_size: incoming_type_size }),
                NumericType::Signed { bit_size: target_type_size },
            ) => {
                match target_type_size.cmp(incoming_type_size) {
                    std::cmp::Ordering::Less => {
                        // If target size is smaller, we do a truncation
                        self.builder.insert_truncate(value, target_type_size, *incoming_type_size)
                    }
                    std::cmp::Ordering::Equal => value,
                    std::cmp::Ordering::Greater => {
                        self.sign_extend(value, *incoming_type_size, target_type_size, location)
                    }
                }
            }
            (
                Type::Numeric(NumericType::Unsigned { bit_size: incoming_type_size }),
                NumericType::Unsigned { bit_size: target_type_size },
            ) => {
                // If target size is smaller, we do a truncation
                if target_type_size < *incoming_type_size {
                    self.builder.insert_truncate(value, target_type_size, *incoming_type_size)
                } else {
                    value
                }
            }
            // When casting a signed value to u1 we can truncate then cast
            (
                Type::Numeric(NumericType::Signed { bit_size: incoming_type_size }),
                NumericType::Unsigned { bit_size: 1 },
            ) => self.builder.insert_truncate(value, 1, *incoming_type_size),

            // For mixed signed to unsigned:
            (
                Type::Numeric(NumericType::Signed { bit_size: incoming_type_size }),
                NumericType::Unsigned { bit_size: target_type_size },
            ) => {
                // when going from lower to higher bit size:
                // 1. we sign-extend to the target bits
                // 2. we are already in the target signedness
                if *incoming_type_size < target_type_size {
                    // By not the casting to a signed type with the target bit size, we avoid potentially going
                    // through i128, which is not a type we support in the frontend, and would be strange in SSA.
                    self.sign_extend(value, *incoming_type_size, target_type_size, location)
                }
                // when the target bit size is not higher than the source:
                // 1. we cast to the required type using the same signedness
                // 2. then we switch the signedness
                else if *incoming_type_size != target_type_size {
                    self.insert_safe_cast(value, NumericType::signed(target_type_size), location)
                } else {
                    value
                }
            }

            // For mixed unsigned to signed:
            // 1. we cast to the required type using the same signedness
            // 2. then we switch the signedness
            (
                Type::Numeric(NumericType::Unsigned { bit_size: incoming_type_size }),
                NumericType::Signed { bit_size: target_type_size },
            ) => {
                if *incoming_type_size != target_type_size {
                    self.insert_safe_cast(value, NumericType::unsigned(target_type_size), location)
                } else {
                    value
                }
            }

            // Field to signed/unsigned:
            (
                Type::Numeric(NumericType::NativeField),
                NumericType::Unsigned { bit_size: target_type_size },
            )
            | (
                Type::Numeric(NumericType::NativeField),
                NumericType::Signed { bit_size: target_type_size },
            ) => {
                self.builder.insert_truncate(value, target_type_size, FieldElement::max_num_bits())
            }
            _ => unreachable!("Invalid cast from {} to {}", incoming_type, typ),
        };
        self.builder.insert_cast(result, typ)
    }

    /// During casting signed values, if target size is bigger, we do a sign extension:
    ///
    /// When the value is negative, it is represented in 2-complement form; `2^s-v`, where `s` is the incoming bit size and `v` is the absolute value.
    /// Sign extension in this case will give `2^t-v`, where `t` is the target bit size.
    /// So we simply convert `2^s-v` into `2^t-v` by adding `2^t-2^s` to the value when the value is negative.
    ///
    /// Casting s-bits signed v0 to t-bits will add the following instructions:
    /// ```ssa
    /// v1 = cast v0 to 's-bits unsigned'
    /// v2 = lt v1, 2**(s-1)
    /// v3 = not(v1)
    /// v4 = cast v3 to 't-bits unsigned'
    /// v5 = v3 * (2**t - 2**s)
    /// v6 = cast v1 to 't-bits unsigned'
    /// return v6 + v5
    /// ```
    ///
    /// Return an unsigned value that we can cast back to the signed type if we want,
    /// or keep it as it is, if we did the sign extension as part of casting e.g. `i8` to `u64`.
    fn sign_extend(
        &mut self,
        value: ValueId,
        incoming_type_size: u32,
        target_type_size: u32,
        location: Location,
    ) -> ValueId {
        let value_as_unsigned =
            self.insert_safe_cast(value, NumericType::unsigned(incoming_type_size), location);
        let half_width = self.builder.numeric_constant(
            FieldElement::from(2_u128.pow(incoming_type_size - 1)),
            NumericType::unsigned(incoming_type_size),
        );
        // value_sign is 1 if the value is positive, 0 otherwise
        let value_sign = self.builder.insert_binary(value_as_unsigned, BinaryOp::Lt, half_width);
        let max_for_incoming_type_size =
            if incoming_type_size == 128 { u128::MAX } else { 2_u128.pow(incoming_type_size) - 1 };
        let max_for_target_type_size =
            if target_type_size == 128 { u128::MAX } else { 2_u128.pow(target_type_size) - 1 };
        let patch = self.builder.numeric_constant(
            FieldElement::from(max_for_target_type_size - max_for_incoming_type_size),
            NumericType::unsigned(target_type_size),
        );
        let mut is_negative_predicate = self.builder.insert_not(value_sign);
        is_negative_predicate = self.insert_safe_cast(
            is_negative_predicate,
            NumericType::unsigned(target_type_size),
            location,
        );
        // multiplication by a boolean cannot overflow
        let patch_with_sign_predicate = self.builder.insert_binary(
            patch,
            BinaryOp::Mul { unchecked: true },
            is_negative_predicate,
        );
        let value_as_unsigned =
            self.builder.insert_cast(value_as_unsigned, NumericType::unsigned(target_type_size));
        // Patch the bit sign, which gives a `target_type_size` bit size value, so it does not overflow.
        self.builder.insert_binary(
            patch_with_sign_predicate,
            BinaryOp::Add { unchecked: true },
            value_as_unsigned,
        )
    }

    /// Create a const offset of an address for an array load or store
    pub(super) fn make_offset(
        &mut self,
        mut address: ValueId,
        offset: u128,
        unchecked: bool,
    ) -> ValueId {
        if offset != 0 {
            let typ = self.builder.type_of_value(address).unwrap_numeric();
            let offset = self.builder.numeric_constant(offset, typ);
            address = self.builder.insert_binary(address, BinaryOp::Add { unchecked }, offset);
        }
        address
    }

    /// Array indexes are u32. This function casts values used as indexes to u32.
    pub(super) fn make_array_index(&mut self, index: ValueId) -> ValueId {
        self.builder.insert_cast(index, NumericType::length_type())
    }

    /// Define a local variable to be some Values that can later be retrieved
    /// by calling self.lookup(id)
    pub(super) fn define(&mut self, id: LocalId, value: Values) {
        let existing = self.definitions.insert(id, value);
        assert!(existing.is_none(), "Variable {id:?} was defined twice in ssa-gen pass");
    }

    /// Looks up the value of a given local variable. Expects the variable to have
    /// been previously defined or panics otherwise.
    pub(super) fn lookup(&self, id: LocalId) -> Values {
        self.definitions
            .get(&id)
            .unwrap_or_else(|| panic!("lookup: variable {id:?} not defined"))
            .clone()
    }

    pub(super) fn lookup_global(&self, id: GlobalId) -> Values {
        self.shared_context
            .globals
            .get(&id)
            .unwrap_or_else(|| panic!("lookup_global: variable {id:?} not defined"))
            .clone()
    }

    /// Extract the given field of the tuple. Panics if the given Values is not
    /// a Tree::Branch or does not have enough fields.
    pub(super) fn get_field(tuple: Values, field_index: usize) -> Values {
        match tuple {
            Tree::Branch(mut trees) => trees.remove(field_index),
            Tree::Leaf(value) => {
                unreachable!("Tried to extract tuple index {field_index} from non-tuple {value:?}")
            }
        }
    }

    /// Extract the given field of the tuple by reference. Panics if the given Values is not
    /// a Tree::Branch or does not have enough fields.
    pub(super) fn get_field_ref(tuple: &Values, field_index: usize) -> &Values {
        match tuple {
            Tree::Branch(trees) => &trees[field_index],
            Tree::Leaf(value) => {
                unreachable!("Tried to extract tuple index {field_index} from non-tuple {value:?}")
            }
        }
    }

    /// Replace the given field of the tuple with a new one. Panics if the given Values is not
    /// a Tree::Branch or does not have enough fields.
    pub(super) fn replace_field(tuple: Values, field_index: usize, new_value: Values) -> Values {
        match tuple {
            Tree::Branch(mut trees) => {
                trees[field_index] = new_value;
                Tree::Branch(trees)
            }
            Tree::Leaf(value) => {
                unreachable!("Tried to extract tuple index {field_index} from non-tuple {value:?}")
            }
        }
    }

    /// Retrieves the given function, adding it to the function queue
    /// if it is not yet compiled.
    pub(super) fn get_or_queue_function(&mut self, id: FuncId) -> Values {
        let function = self.shared_context.get_or_queue_function(id);
        self.builder.import_function(function).into()
    }

    /// Extracts the current value out of an LValue.
    ///
    /// Goal: Handle the case of assigning to nested expressions such as `foo.bar[i1].baz[i2] = e`
    ///       while also noting that assigning to arrays will create a new array rather than mutate
    ///       the original.
    ///
    /// Method: First `extract_current_value` must recurse on the lvalue to extract the current
    ///         value contained:
    ///
    /// ```text
    /// v0 = foo.bar                 ; allocate instruction for bar
    /// v1 = load v0                 ; loading the bar array
    /// v2 = add i1, baz_index       ; field offset for index i1, field baz
    /// v3 = array_get v1, index v2  ; foo.bar[i1].baz
    /// ```
    ///
    /// Method (part 2): Then, `assign_new_value` will recurse in the opposite direction to
    ///                  construct the larger value as needed until we can `store` to the nearest
    ///                  allocation.
    ///
    /// v4 = array_set v3, index i2, e   ; finally create a new array setting the desired value
    /// v5 = array_set v1, index v2, v4  ; now must also create the new bar array
    /// store v5 in v0                   ; and store the result in the only mutable reference
    ///
    /// The returned `LValueRef` tracks the current value at each step of the lvalue.
    /// This is later used by `assign_new_value` to construct a new updated value that
    /// can be assigned to an allocation within the LValueRef::Ident.
    ///
    /// This is operationally equivalent to extract_current_value_recursive, but splitting these
    /// into two separate functions avoids cloning the outermost `Values` returned by the recursive
    /// version, as it is only needed for recursion.
    pub(super) fn extract_current_value(
        &mut self,
        lvalue: &ast::LValue,
    ) -> Result<LValue, RuntimeError> {
        Ok(match lvalue {
            ast::LValue::Ident(ident) => {
                let (reference, should_auto_deref) = self.ident_lvalue(ident);
                if should_auto_deref { LValue::Dereference { reference } } else { LValue::Ident }
            }
            ast::LValue::Index { array, index, location, .. } => {
                self.index_lvalue(array, index, location)?.2
            }
            ast::LValue::MemberAccess { object, field_index } => {
                let (old_object, object_lvalue) = self.extract_current_value_recursive(object)?;
                let object_lvalue = Box::new(object_lvalue);
                LValue::MemberAccess { old_object, object_lvalue, index: *field_index }
            }
            ast::LValue::Dereference { reference, .. } => {
                let (reference, _) = self.extract_current_value_recursive(reference)?;
                LValue::Dereference { reference }
            }
            ast::LValue::Clone(lvalue) => self.extract_current_value(lvalue)?,
        })
    }

    fn dereference_lvalue(&mut self, values: &Values, element_type: &ast::Type) -> Values {
        let element_types = Self::convert_type(element_type);
        values.map_both(element_types, |value, element_type| {
            let reference = value.eval_reference();
            self.builder.insert_load(reference, element_type).into()
        })
    }

    /// Compile the given identifier as a reference - ie. avoid calling .eval().
    /// Returns the variable's value and whether the variable is mutable.
    fn ident_lvalue(&self, ident: &ast::Ident) -> (Values, bool) {
        match &ident.definition {
            ast::Definition::Local(id) => (self.lookup(*id), ident.mutable),
            other => panic!("Unexpected definition found for mutable value: {other}"),
        }
    }

    /// Compile the given `array[index]` expression as a reference.
    /// This will return a triple of (array, index, lvalue_ref, `Option<length>`) where the lvalue_ref records the
    /// structure of the lvalue expression for use by `assign_new_value`.
    /// The optional length is for indexing vectors rather than arrays since vectors
    /// are represented as a tuple in the form: (length, vector contents).
    fn index_lvalue(
        &mut self,
        array: &ast::LValue,
        index: &ast::Expression,
        location: &Location,
    ) -> Result<(ValueId, ValueId, LValue, Option<ValueId>), RuntimeError> {
        let (old_array, array_lvalue) = self.extract_current_value_recursive(array)?;
        let index = self.codegen_non_tuple_expression(index)?;
        let array_lvalue = Box::new(array_lvalue);
        let array_values = old_array.clone().into_value_vector(self);

        let location = *location;
        // A vector is represented as a tuple (length, vector contents).
        // We need to fetch the second value.
        Ok(if array_values.len() > 1 {
            let vector_lvalue = LValue::VectorIndex {
                old_vector: old_array,
                index,
                vector_lvalue: array_lvalue,
                location,
            };
            (array_values[1], index, vector_lvalue, Some(array_values[0]))
        } else {
            let array_lvalue =
                LValue::Index { old_array: array_values[0], index, array_lvalue, location };
            (array_values[0], index, array_lvalue, None)
        })
    }

    fn extract_current_value_recursive(
        &mut self,
        lvalue: &ast::LValue,
    ) -> Result<(Values, LValue), RuntimeError> {
        match lvalue {
            ast::LValue::Ident(ident) => {
                let (variable, should_auto_deref) = self.ident_lvalue(ident);
                if should_auto_deref {
                    let dereferenced = self.dereference_lvalue(&variable, &ident.typ);
                    Ok((dereferenced, LValue::Dereference { reference: variable }))
                } else {
                    Ok((variable.clone(), LValue::Ident))
                }
            }
            ast::LValue::Index { array, index, element_type, location } => {
                let (old_array, index, index_lvalue, max_length) =
                    self.index_lvalue(array, index, location)?;
                let element = self.codegen_array_index(
                    old_array,
                    index,
                    element_type,
                    *location,
                    max_length,
                )?;
                Ok((element, index_lvalue))
            }
            ast::LValue::MemberAccess { object, field_index: index } => {
                let (old_object, object_lvalue) = self.extract_current_value_recursive(object)?;
                let object_lvalue = Box::new(object_lvalue);
                let element = Self::get_field_ref(&old_object, *index).clone();
                Ok((element, LValue::MemberAccess { old_object, object_lvalue, index: *index }))
            }
            ast::LValue::Dereference { reference, element_type } => {
                let (reference, _) = self.extract_current_value_recursive(reference)?;
                let dereferenced = self.dereference_lvalue(&reference, element_type);
                Ok((dereferenced, LValue::Dereference { reference }))
            }
            ast::LValue::Clone(lvalue) => {
                let (values, lvalue) = self.extract_current_value_recursive(lvalue)?;
                values.clone().for_each(|value| {
                    let value = value.eval(self);
                    self.builder.increment_array_reference_count(value);
                });
                Ok((values, lvalue))
            }
        }
    }

    /// Assigns a new value to the given LValue.
    /// The LValue can be created via a previous call to extract_current_value.
    /// This method recurs on the given LValue to create a new value to assign an allocation
    /// instruction within an LValue::Ident or LValue::Dereference - see the comment on
    /// `extract_current_value` for more details.
    pub(super) fn assign_new_value(&mut self, lvalue: LValue, new_value: Values) {
        match lvalue {
            LValue::Ident => unreachable!("Cannot assign to a variable without a reference"),
            LValue::Index { old_array: mut array, index, array_lvalue, location } => {
                let array_type = &self.builder.type_of_value(array);

                // Checks for index Out-of-bounds
                match array_type {
                    Type::Array(_, len) => {
                        // Out of bounds array accesses are guaranteed to fail in ACIR so this check is performed implicitly.
                        // We then only need to inject it for brillig functions.
                        if self.builder.current_function.runtime().is_brillig() {
                            let len = self
                                .builder
                                .numeric_constant(u128::from(*len), NumericType::length_type());
                            self.codegen_access_check(index, len);
                        }
                    }
                    _ => unreachable!("must have array or vector but got {array_type}"),
                }

                array = self.assign_lvalue_index(new_value, array, index, location);
                self.assign_new_value(*array_lvalue, array.into());
            }
            LValue::VectorIndex { old_vector: vector, index, vector_lvalue, location } => {
                let mut vector_values = vector.into_value_vector(self);

                let array_type = &self.builder.type_of_value(vector_values[1]);

                // Checks for index Out-of-bounds
                match array_type {
                    Type::Vector(_) => {
                        self.codegen_access_check(index, vector_values[0]);
                    }
                    _ => unreachable!("must have array or vector but got {array_type}"),
                }

                vector_values[1] =
                    self.assign_lvalue_index(new_value, vector_values[1], index, location);

                // The size of the vector does not change in a vector index assignment so we can reuse the same length value
                let new_vector = Tree::Branch(vec![vector_values[0].into(), vector_values[1].into()]);
                self.assign_new_value(*vector_lvalue, new_vector);
            }
            LValue::MemberAccess { old_object, index, object_lvalue } => {
                let new_object = Self::replace_field(old_object, index, new_value);
                self.assign_new_value(*object_lvalue, new_object);
            }
            LValue::Dereference { reference } => {
                self.assign(reference, new_value);
            }
        }
    }

    fn assign_lvalue_index(
        &mut self,
        new_value: Values,
        mut array: ValueId,
        index: ValueId,
        location: Location,
    ) -> ValueId {
        let index = self.make_array_index(index);
        let element_size =
            self.builder.numeric_constant(self.element_size(array), NumericType::length_type());

        // The actual base index is the user's index * the array element type's size
        // Unchecked mul because we are reaching for an array element: if it overflows here
        // it would have overflowed when creating the array.
        let mut index = self.builder.set_location(location).insert_binary(
            index,
            BinaryOp::Mul { unchecked: true },
            element_size,
        );
        let one = self.builder.numeric_constant(FieldElement::one(), NumericType::length_type());

        new_value.for_each(|value| {
            let value = value.eval(self);
            let mutable = false;
            array = self.builder.insert_array_set(array, index, value, mutable);
            // Unchecked add because this can't overflow (it would have overflowed when creating the array)
            index = self.builder.insert_binary(index, BinaryOp::Add { unchecked: true }, one);
        });
        array
    }

    fn element_size(&self, array: ValueId) -> FieldElement {
        let size = self.builder.type_of_value(array).element_size();
        FieldElement::from(size as u128)
    }

    /// Given an lhs containing only references, create a store instruction to store each value of
    /// rhs into its corresponding value in lhs.
    fn assign(&mut self, lhs: Values, rhs: Values) {
        match (lhs, rhs) {
            (Tree::Branch(lhs_branches), Tree::Branch(rhs_branches)) => {
                assert_eq!(lhs_branches.len(), rhs_branches.len());

                for (lhs, rhs) in lhs_branches.into_iter().zip(rhs_branches) {
                    self.assign(lhs, rhs);
                }
            }
            (Tree::Leaf(lhs), Tree::Leaf(rhs)) => {
                let (lhs, rhs) = (lhs.eval_reference(), rhs.eval(self));
                self.builder.insert_store(lhs, rhs);
            }
            (lhs, rhs) => {
                unreachable!(
                    "assign: Expected lhs and rhs values to match but found {lhs:?} and {rhs:?}"
                )
            }
        }
    }

    pub(crate) fn enter_loop(&mut self, loop_: Loop) {
        self.loops.push(loop_);
    }

    pub(crate) fn exit_loop(&mut self) {
        self.loops.pop();
    }

    pub(crate) fn current_loop(&self) -> Loop {
        // The frontend should ensure break/continue are never used outside a loop
        *self.loops.last().expect("current_loop: not in a loop!")
    }
}

/// True if the given operator cannot be encoded directly and needs
/// to be represented as !(some other operator)
fn operator_requires_not(op: BinaryOpKind) -> bool {
    use BinaryOpKind::*;
    matches!(op, NotEqual | LessEqual | GreaterEqual)
}

/// True if the given operator cannot be encoded directly and needs
/// to have its lhs and rhs swapped to be represented with another operator.
/// Example: (a > b) needs to be represented as (b < a)
fn operator_requires_swapped_operands(op: BinaryOpKind) -> bool {
    use BinaryOpKind::*;
    matches!(op, Greater | LessEqual)
}

/// Converts the given operator to the appropriate BinaryOp.
/// Take care when using this to insert a binary instruction: this requires
/// checking operator_requires_not and operator_requires_swapped_operands
/// to represent the full operation correctly.
fn convert_operator(op: BinaryOpKind) -> BinaryOp {
    match op {
        BinaryOpKind::Add => BinaryOp::Add { unchecked: false },
        BinaryOpKind::Subtract => BinaryOp::Sub { unchecked: false },
        BinaryOpKind::Multiply => BinaryOp::Mul { unchecked: false },
        BinaryOpKind::Divide => BinaryOp::Div,
        BinaryOpKind::Modulo => BinaryOp::Mod,
        BinaryOpKind::Equal => BinaryOp::Eq,
        BinaryOpKind::NotEqual => BinaryOp::Eq, // Requires not
        BinaryOpKind::Less => BinaryOp::Lt,
        BinaryOpKind::Greater => BinaryOp::Lt, // Requires operand swap
        BinaryOpKind::LessEqual => BinaryOp::Lt, // Requires operand swap and not
        BinaryOpKind::GreaterEqual => BinaryOp::Lt, // Requires not
        BinaryOpKind::And => BinaryOp::And,
        BinaryOpKind::Or => BinaryOp::Or,
        BinaryOpKind::Xor => BinaryOp::Xor,
        BinaryOpKind::ShiftLeft => BinaryOp::Shl,
        BinaryOpKind::ShiftRight => BinaryOp::Shr,
    }
}

impl SharedContext {
    /// Create a new SharedContext for the given monomorphized program.
    pub(super) fn new(program: Program) -> Self {
        let globals_shared_context = SharedContext::new_for_globals();

        let globals_id = Program::global_space_id();

        // Queue the function representing the globals space for compilation
        globals_shared_context.get_or_queue_function(globals_id);

        let mut context = FunctionContext::new(
            "globals".to_owned(),
            &vec![],
            RuntimeType::Brillig(InlineType::default()),
            &globals_shared_context,
            GlobalsGraph::default(),
        );
        let mut globals = BTreeMap::default();
        for (id, (_, _, global)) in program.globals.iter() {
            let values = context.codegen_expression(global).unwrap();
            globals.insert(*id, values);
        }

        Self {
            functions: Default::default(),
            function_queue: Default::default(),
            function_counter: Default::default(),
            program,
            globals_context: context.builder.current_function,
            globals,
        }
    }

    pub(super) fn new_for_globals() -> Self {
        let globals_context = Function::new_for_globals();

        Self {
            functions: Default::default(),
            function_queue: Default::default(),
            function_counter: Default::default(),
            program: Default::default(),
            globals_context,
            globals: Default::default(),
        }
    }

    /// Pops the next function from the shared function queue, returning None if the queue is empty.
    pub(super) fn pop_next_function_in_queue(&self) -> Option<(FuncId, IrFunctionId)> {
        self.function_queue.lock().expect("Failed to lock function_queue").pop()
    }

    /// Return the matching id for the given function if known. If it is not known this
    /// will add the function to the queue of functions to compile, assign it a new id,
    /// and return this new id.
    pub(super) fn get_or_queue_function(&self, id: FuncId) -> IrFunctionId {
        // Start a new block to guarantee the destructor for the map lock is released
        // before map needs to be acquired again in self.functions.write() below
        {
            let map = self.functions.read().expect("Failed to read self.functions");
            if let Some(existing_id) = map.get(&id) {
                return *existing_id;
            }
        }

        let next_id = self.function_counter.next();

        let mut queue = self.function_queue.lock().expect("Failed to lock function queue");
        queue.push((id, next_id));

        self.functions.write().expect("Failed to write to self.functions").insert(id, next_id);

        next_id
    }
}

/// Used to remember the results of each step of extracting a value from an ast::LValue
#[derive(Debug)]
pub(super) enum LValue {
    Ident,
    Index { old_array: ValueId, index: ValueId, array_lvalue: Box<LValue>, location: Location },
    VectorIndex { old_vector: Values, index: ValueId, vector_lvalue: Box<LValue>, location: Location },
    MemberAccess { old_object: Values, index: usize, object_lvalue: Box<LValue> },
    Dereference { reference: Values },
}
