#!/usr/bin/env bash
# A wrapper for Earthly that is meant to catch signs of known intermittent failures and continue.
# The silver lining is if Earthly does crash, the cache can pick up the build.
set -eu -o pipefail

MAX_WAIT_TIME=300 # Maximum wait time in seconds
WAIT_INTERVAL=10  # Interval between checks in seconds
elapsed_time=0

OUTPUT_FILE=$(mktemp)
INCONSISTENT_GRAPH_STATE_COUNT=0  # Counter for 'inconsistent graph state' errors

# Maximum attempts
MAX_ATTEMPTS=6
ATTEMPT_COUNT=0

# earthly settings
export EARTHLY_ALLOW_PRIVILEGED=true
export EARTHLY_USE_INLINE_CACHE=true
export EARTHLY_NO_BUILDKIT_UPDATE=true
# make sure earthly gives annotations that github picks up
export GITHUB_ACTIONS=true
export FORCE_COLOR=1
export EARTHLY_CONFIG=$(git rev-parse --show-toplevel)/.github/earthly-ci-config.yml

function wipe_non_cache_docker_state {
  echo "Detected corrupted docker images. Wiping and trying again."
  # Based on https://stackoverflow.com/a/75849307
  # wipe everything but volumes where we have earthly cache
  sudo service docker stop
  sudo bash -c 'rm -rf /var/lib/docker/{buildkit,containers,image,network,overlay2,plugins.runtimes,swarm,tmp,trust}/*'
  # restart docker - might take down builds, but we need to recover anyway
  sudo service docker restart
}

# Handle earthly commands and retries
while [ $ATTEMPT_COUNT -lt $MAX_ATTEMPTS ]; do
  if earthly $@ 2>&1 | tee $OUTPUT_FILE >&2 ; then
    exit 0  # Success, exit the script
  else
    # Increment attempt counter
    ATTEMPT_COUNT=$((ATTEMPT_COUNT + 1))
    echo "Attempt #$ATTEMPT_COUNT failed."

    # Check the output for specific errors
    if grep 'failed to get edge: inconsistent graph state' $OUTPUT_FILE >/dev/null || grep 'failed to get state for index' $OUTPUT_FILE >/dev/null ; then
      INCONSISTENT_GRAPH_STATE_COUNT=$((INCONSISTENT_GRAPH_STATE_COUNT + 1))
      if [ "$INCONSISTENT_GRAPH_STATE_COUNT" -eq $MAX_ATTEMPTS ]; then
        echo "Unable to recover from 'inconsistent graph state' or 'failed to get state for index'. Do something to change the earthly cache state, like merging master or just retrying after other things build. If all else fails, connect to runner with ci.py and run 'earthly prune' for a bit (can cancel early)."
        exit 1
      fi
      echo "Got 'inconsistent graph state' or 'failed to get state for index'. Sleeping for 30 seconds and retrying."
      sleep 30
    elif grep 'Error: pull ping error: pull ping response' $OUTPUT_FILE >/dev/null; then
      echo "Got 'Error: pull ping error: pull ping response', intermittent failure when writing out images to docker. If this persists, try 'systemctl restart docker' on the spot instance."
    elif grep '================================= System Info ==================================' $OUTPUT_FILE >/dev/null || grep 'Error response from daemon: removal of container earthly-buildkitd is already in progress: exit status 1' $OUTPUT_FILE >/dev/null ; then
      echo "Detected an Earthly daemon restart, possibly due to it (mis)detecting a cache setting change, trying again..."
    elif grep 'dial unix /run/buildkit/buildkitd.sock' $OUTPUT_FILE >/dev/null; then
      echo "Detected earthly unable to find buildkit, waiting and trying again..."
      sleep 20
    elif grep 'The container name "/earthly-buildkitd" is already in use by container' $OUTPUT_FILE >/dev/null; then
      if [ $ATTEMPT_COUNT -lt 3 ] ; then
        echo "Detected earthly bootstrap happening in parallel and failing, waiting and trying again."
      else
        wipe_non_cache_docker_state
      fi
      sleep 20
    elif grep 'status 125: docker: Error response from daemon: layer does not exist.' $OUTPUT_FILE >/dev/null || grep 'could not determine buildkit address - is Docker or Podman running?' $OUTPUT_FILE >/dev/null || grep 'please make sure the URL is valid, and Docker 18.09 or later is installed on the remote host' $OUTPUT_FILE >/dev/null ; then
      wipe_non_cache_docker_state
      # wait for other docker restarts
      sleep 20
    else
      # If other errors, exit the script
      echo "Errors may exist in other jobs. Please see the run summary page and check for Build Summary. If there are no errors, it may be because runs were interrupted due to runner going down (please report this)."
      exit 1
    fi
  fi
done

echo "Maximum attempts reached, exiting with failure."
exit 1
